{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifiers\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree # for Decision Tree\n",
    "\n",
    "# Pre-processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics and Validation\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom imports\n",
    "from util.pickler import pickle_out\n",
    "from util.author import results2csv\n",
    "from util.fe import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Precessed Data\n",
    "train = pd.read_csv('data/train_processed_1.csv', index_col='PassengerId')\n",
    "test = pd.read_csv('data/test_processed_1.csv', index_col='PassengerId')\n",
    "\n",
    "# Feature Engineering\n",
    "train = pd.get_dummies(data=train, drop_first=True)\n",
    "test = pd.get_dummies(data=test, drop_first=True)\n",
    "\n",
    "# Split Datasets\n",
    "train_y = train.pop('Survived')\n",
    "train_x = train\n",
    "test_x = test # Nothing to split! Test-set has no target columns.\n",
    "\n",
    "# select_colns = ['Pclass', 'RoundedFare', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "select_colns = ['Pclass', 'Age', 'GroupCount', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "train_x = transform(train_x, select_colns)\n",
    "test_x = transform(test_x, select_colns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Method : Bagging (aka Bootstrap Aggregator) with setting oob_score=False\n",
    "* With Bagging, not all data is used for training but only a subset of data is used by each of the base_estimator.\n",
    "* With the reamining unused training data, you can cross-validate it inside andd this is called OOB (Out Of Bag) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle The classifier we want to use with Ensemble\n",
    "odtclf = pickle_out('pickle/optimized_dtree_clf.pkl') # Not reqd. Just create a new object of your dedsired classifier class\n",
    "# odtclf = tree.DecisionTreeClassifier(random_state=42) # You can't tune the params of this estimator\n",
    "\n",
    "bagclf = BaggingClassifier(base_estimator=odtclf, \n",
    "                           bootstrap=True, # Sampling with replacement\n",
    "                           n_jobs=-1, # Use all cores\n",
    "                           verbose=1,\n",
    "                           random_state=42)\n",
    "\n",
    "# Possible params for estimator=BaggingClassifier\n",
    "params = {\n",
    "    'n_estimators': [10,15,20,25,50],\n",
    "    'max_samples': [0.4,0.6,0.8,0.9,1.0]\n",
    "}\n",
    "\n",
    "gscv = GridSearchCV(estimator=bagclf, \n",
    "                    cv=3,\n",
    "                    param_grid=params, \n",
    "                    n_jobs=-1, \n",
    "                    verbose=1)\n",
    "gscv.fit(train_x, train_y)\n",
    "print('Grid Scores:\\n',gscv.grid_scores_) # Debugging: Best Score = Check out for highest mean-scores with least Std-deviation.\n",
    "\n",
    "best_score = gscv.best_score_\n",
    "print('Best Score : ', best_score)\n",
    "'''\n",
    "Best Score :  0.8148148148148148\n",
    "\tmax_samples \t 0.4\n",
    "\tn_estimators \t 20\n",
    "'''\n",
    "best_params = gscv.best_estimator_.get_params()\n",
    "for k in sorted(params.keys()):\n",
    "    print('\\t{0} \\t {1}'.format(k, best_params[k]))\n",
    "\n",
    "train_y_pred = gscv.predict(train_x)\n",
    "print('accuracy_score = ',accuracy_score(train_y, train_y_pred))\n",
    "print('\\n confusion_matrix :\\n', confusion_matrix(train_y, train_y_pred))\n",
    "print('\\n classification_report: \\n', classification_report(train_y, train_y_pred))\n",
    "\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(gscv, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('\\n cross_val_score : \\n', cv_scores)\n",
    "# [0.7979798  0.81144781 0.81144781] with n_estimators=10\n",
    "# [0.8047138 0.8047138 0.7979798] with n_estimators=100\n",
    "# [0.8013468 0.8013468 0.8013468] with n_estimators=500\n",
    "# [0.82089552 0.8358209  0.81343284] with gridsearch\n",
    "# [0.80223881 0.83208955 0.79104478] with all columns selected\n",
    "# [0.77985075 0.83208955 0.80597015] with columns - 'RoundedFare', 'SibSp', 'Parch' - removed\n",
    "# [0.81343284 0.82462687 0.80970149] with column 'GroupCount' replacing 'SibSp', 'Parch'. and removing 'RoundedFare' \n",
    "\n",
    "# Make Predictions\n",
    "test_y_pred = gscv.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/bagging_gridsearch.csv\"\n",
    "results2csv(test_x.index, test_y_pred, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Method : Bagging (aka Bootstrap Aggregator) with setting oob_score=True\n",
    "* With Bagging, not all data is used for training but only a subset of data is used by each of the base_estimator.\n",
    "* With the reamining unused training data, you can cross-validate it inside andd this is called OOB (Out Of Bag) score. For this, you set OOB=True and check its score later like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree # for Decision Tree\n",
    "\n",
    "# Unpickle The classifier we want to use with Ensemble\n",
    "# odtclf = pickle_out('pickle/optimized_dtree_clf.pkl') # Not reqd. Just create a new object of your dedsired classifier class\n",
    "odtclf = tree.DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "bagclf = BaggingClassifier(base_estimator=odtclf, \n",
    "                           n_estimators=20, # Number of base estimators\n",
    "                           bootstrap=True, # Sampling with replacement\n",
    "                           n_jobs=-1, # Use all cores\n",
    "                           oob_score=True,\n",
    "#                            verbose=1,\n",
    "                           random_state=42)\n",
    "bagclf.fit(train_x, train_y)\n",
    "\n",
    "# Check OOB  Score\n",
    "print('OOB Score : ', bagclf.oob_score_)\n",
    "\n",
    "train_y_pred = bagclf.predict(train_x)\n",
    "print('accuracy_score = ',accuracy_score(train_y, train_y_pred))\n",
    "print('\\n confusion_matrix :\\n', confusion_matrix(train_y, train_y_pred))\n",
    "print('\\n classification_report: \\n', classification_report(train_y, train_y_pred))\n",
    "\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(bagclf, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('\\n cross_val_score : \\n', cv_scores)\n",
    "# [0.7979798  0.81144781 0.81144781] with n_estimators=10\n",
    "# [0.8047138 0.8047138 0.7979798] with n_estimators=100\n",
    "# [0.8013468 0.8013468 0.8013468] with n_estimators=500 \n",
    "# [0.79104478 0.79477612 0.80597015] with all columns selected\n",
    "# [0.75746269 0.8358209  0.79850746] with columns - 'RoundedFare', 'SibSp', 'Parch' - removed\n",
    "\n",
    "# Make Predictions\n",
    "test_y_pred = bagclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/bagging.csv\"\n",
    "results2csv(test_x.index, test_y_pred, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Survival - Dead/Alive\n",
    "Just wanted to visualaize survival in scatterplot for intuitiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "# pca = PCA(n_components=2, svd_solver='full')\n",
    "# PCA(copy=True, iterated_power='auto', n_components=2, random_state=None, svd_solver='full', tol=0.0, whiten=False)\n",
    "pca.fit(train_x)\n",
    "print('explained_variance_ratio_ : ',pca.explained_variance_ratio_)  \n",
    "print('singular_values_ : ', pca.singular_values_)\n",
    "X = pd.DataFrame(pca.transform(train_x), \n",
    "                 index=train_x.index)\n",
    "y = train_y.copy()\n",
    "\n",
    "fig = plt.figure(1, figsize=(6, 6))\n",
    "plt.clf() # clear currrent figure to clean memory footprint\n",
    "ax = plt.gca() # gca = get current axes\n",
    "\n",
    "# # Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(train_y, [0,1]).astype(np.float)\n",
    "colors = ['red','green']\n",
    "labels = ['Dead', 'Alive']\n",
    "for k in [0,1]:\n",
    "    rows = y.loc[y==k].index.values\n",
    "    plt.scatter(X.loc[rows, 0], X.loc[rows, 1], c=colors[k], label=labels[k], alpha=0.25)\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Observation: PCA with just 2 dimensions isn't helping see distinction. The survival is so mixed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]\n",
    "for i in range(1,train_x.shape[1]+1):\n",
    "    lst.append(\"PC_{0}\".format(i))\n",
    "pca = PCA(n_components=(train_x.shape[1]))\n",
    "pca.fit(train_x)\n",
    "pd.DataFrame(data=pca.components_, columns=train_x.columns, index=lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
