{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVE : Beat the baseline accuracy of ~78.57% (See A_*.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm # for SVM classifier\n",
    "from sklearn import tree # for Decision Tree\n",
    "\n",
    "# Pre-processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Metrics and Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Utility Packages used in this file\n",
    "from util.math import round_single, round_double, range_simple\n",
    "from util.plot_helper import make_meshgrid, plot_contours\n",
    "from util.pickler import pickle_in\n",
    "from util.author import results2csv\n",
    "from util.fe import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Feature Engineering Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_processed_1.csv', index_col='PassengerId')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_processed_1.csv', index_col='PassengerId')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Shape :', train.shape)\n",
    "print('Test Shape :', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(data=train, drop_first=True)\n",
    "print('Train Shape :', train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(data=test, drop_first=True)\n",
    "print('Test Shape :', test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 : Split Datasets as x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train.pop('Survived')\n",
    "train_x = train\n",
    "print('train_x shape :', train_x.shape)\n",
    "print('train_y shape :', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test # Test-set has no target columns\n",
    "print('test_x shape :', test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 : Pre-processing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_colns = ['Pclass', 'RoundedFare', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S'] # Removing 'RoundedFare' yields better accuracy\n",
    "\n",
    "train_xx = transform(train_x, select_colns)\n",
    "test_xx = transform(test_x, select_colns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 : Data Modelling with Logistic Regression Classifier (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrclf = LogisticRegression(random_state=42)\n",
    "lrclf.fit(train_x,train_y)\n",
    "\n",
    "cv_scores = cross_val_score(lrclf, train_x, train_y, cv=3, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores) # [0.79124579 0.8047138  0.79124579]\n",
    "\n",
    "results = lrclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "results2csv(test_x.index, results, 'data/predictions/logistic_regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 : Data Modelling with SGDClassifier (default params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default the param loss='hinge'. When the loss function is 'hinge', it gives linear SVM.\n",
    "# This one below thus gives Linear SVM model\n",
    "sgdclf = SGDClassifier(random_state=42, max_iter=100)\n",
    "sgdclf.fit(train_x,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv_scores = cross_val_score(sgdclf, train_x, train_y, cv=3, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores) # [0.76767677 0.71043771 0.79124579]\n",
    "\n",
    "sgd_results = sgdclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "results2csv(test_x.index, sgd_results, 'data/predictions/sgd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 : Data Modelling with Logistic Regression Classifier (custom params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrclf = LogisticRegression(random_state=42, max_iter=300, C=0.3, solver='sag',n_jobs=3) # C=0.3 maade the real difference here\n",
    "lrclf.fit(train_xx,train_y)\n",
    "\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(lrclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores) \n",
    "# [0.79124579 0.8047138  0.79124579] # Default params and without StandardScaler preprocessing\n",
    "# [0.78787879 0.79124579 0.8047138 ] # With just StandardScaler preprocessing\n",
    "# [0.79124579 0.8047138  0.8013468 ] # With StandardScaler preprocessing and Custom Params\n",
    "# [0.80970149 0.79850746 0.80223881] # With StandardScaler preprocessing, Custom Params and ShuffleSplit cv-strategy\n",
    "\n",
    "# Make Predictions\n",
    "results = lrclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "results2csv(test_x.index, results, 'data/predictions/logistic_regression_tuned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = pd.Series(data=lrclf.coef_.flatten(),index=select_colns)\n",
    "coeffs\n",
    "# Gosh, Pclass and Sex seem to have got least importance and RoundedFare got highest importance :facepalm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 : Data Modelling with SGDClassifier (custom params) giving Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default the param loss='hinge'. When the loss function is 'hinge', it gives linear SVM.\n",
    "sgdclf = SGDClassifier(random_state=42, max_iter=1000, alpha=0.7)\n",
    "sgdclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(sgdclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores) \n",
    "# [0.76767677 0.71043771 0.79124579] # with default params\n",
    "# [0.79104478 0.79850746 0.82089552] # With SS preprocessing, 1k iterations\n",
    "# [0.82462687 0.82089552 0.79104478] # With SS preprocessing, 1k iterations, alpha=0.7, default loss=hinge\n",
    "\n",
    "# Make Predictions\n",
    "sgd_results = sgdclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "loss_function_name = sgdclf.loss_function_.__class__.__name__.lower()\n",
    "fname = \"data/predictions/sgd_tuned_with_{0}.csv\".format(loss_function_name)\n",
    "results2csv(test_x.index, sgd_results, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 : Data Modelling with SGDClassifier (custom params) giving Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default the param loss='hinge'. When the loss function is 'hinge', it gives linear SVM.\n",
    "# When the loss function is set to 'log', it gives Logistic Regression\n",
    "# For other loss functions see http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "# sgdclf = SGDClassifier(random_state=42, max_iter=1000, alpha=0.7)\n",
    "sgdclf = SGDClassifier(random_state=42, max_iter=5000, alpha=0.25, loss='log')\n",
    "sgdclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(sgdclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores) \n",
    "# [0.76767677 0.71043771 0.79124579] # with default params\n",
    "# [0.79104478 0.79850746 0.82089552] # With SS preprocessing, 1k iterations\n",
    "# [0.82462687 0.82089552 0.79104478] # With SS preprocessing, 1k iterations, alpha=0.7, default loss=hinge\n",
    "# [0.80970149 0.82835821 0.82462687] # With SS preprocessing, 1k iterations, alpha=0.7, loss=log\n",
    "\n",
    "# Make Predictions\n",
    "sgd_results = sgdclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "loss_function_name = sgdclf.loss_function_.__class__.__name__.lower()\n",
    "fname = \"data/predictions/sgd_tuned_with_{0}.csv\".format(loss_function_name)\n",
    "results2csv(test_x.index, sgd_results, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 : Data Modelling with SVM Classifier - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= 1.0 #0.1 \n",
    "svmclf = svm.SVC(kernel='linear', C=C, random_state=42)\n",
    "# svmclf = svm.SVC(kernel='linear', C=C, random_state=42, class_weight={1:2})\n",
    "svmclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(svmclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores) \n",
    "# [0.79104478 0.79477612 0.79104478]\n",
    "\n",
    "# Make Predictions\n",
    "svm_results = svmclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "kernel_name = svmclf.kernel\n",
    "fname = \"data/predictions/svm_with_{0}_kernel.csv\".format(kernel_name)\n",
    "results2csv(test_x.index, svm_results, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9 : Data Modelling with SVM Classifier - Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= 0.75 #1.0\n",
    "svmclf = svm.SVC(kernel='poly', degree=3, C=C, random_state=42)\n",
    "# svmclf = svm.SVC(kernel='poly', degree=3, C=C, random_state=42, class_weight={0:3, 1:5})\n",
    "svmclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(svmclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# [0.81716418 0.82462687 0.81716418] # When degree=2, class_weight is default  and C=0.75 \n",
    "# [0.80223881 0.81343284 0.8358209 ] # When degree=3, class_weight is default  and C=0.75 \n",
    "# [0.80597015 0.80597015 0.80970149] # When degree=3, class_weight={0:3, 1:5} and C=0.75 \n",
    "\n",
    "# Make Predictions\n",
    "svm_results = svmclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "kernel_name = svmclf.kernel\n",
    "fname = \"data/predictions/svm_with_{0}_kernel.csv\".format(kernel_name)\n",
    "results2csv(test_x.index, svm_results, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10 : Data Modelling with SVM Classifier - Gaussian Radial Basis Function (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select_colns = ['Pclass','RoundedFare', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "# select_colns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "select_colns = ['Pclass', 'Age', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "\n",
    "train_xx = transform(train_x, select_colns) #train_xx.loc[:, select_colns]\n",
    "test_xx = transform(test_x, select_colns) #test_xx.loc[:, select_colns]\n",
    "\n",
    "svmclf = svm.SVC(kernel='rbf', gamma=.25, C=.5, random_state=42)\n",
    "svmclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(svmclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# [0.81343284 0.8358209  0.82462687] # when C=0.75, kernel='rbf', gamma=0.7\n",
    "\n",
    "# Make Predictions\n",
    "svm_results = svmclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "kernel_name = svmclf.kernel\n",
    "fname = \"data/predictions/svm_with_{0}_kernel.csv\".format(kernel_name)\n",
    "results2csv(test_x.index, svm_results, fname)\n",
    "\n",
    "confusion_matrix(train_y, svmclf.predict(train_xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBJECTIVE : Tune SVC with GBF Kernel for better results - An attempt.\n",
    "* Notes *\n",
    "* C and Gamma are the parameters for a nonlinear support vector machine (SVM) with a Gaussian radial basis function kernel.\n",
    "* C is the parameter for the soft margin cost function, which controls the influence of each individual support vector; this process involves trading error penalty for stability.\n",
    "* C controls the cost of misclassification on the training data.\n",
    "* Small C makes the cost of misclassificaiton low (\"soft margin\"), thus allowing more of them for the sake of wider \"cushion\".\n",
    "* Large C makes the cost of misclassification high ('hard margin\"), thus forcing the algorithm to explain the input data stricter and potentially overfit.\n",
    "* The goal is to find the balance between \"not too strict\" and \"not too loose\". Cross-validation and resampling, along with grid search, are good ways to finding the best C.\n",
    "* Gamma is the free parameter of the Gaussian radial basis function.\n",
    "* large gamma leads to high bias and low variance models, and vice-versa.\n",
    "* Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. \n",
    "* The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_colns = ['Pclass','RoundedFare', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "train_xx = transform(train_x, select_colns)\n",
    "test_xx = transform(test_x, select_colns)\n",
    "\n",
    "C= 2.5 # Cost of mis-classification\n",
    "svmclf = svm.SVC(kernel='rbf', gamma=0.05, C=C, random_state=42, class_weight={1:1.25}) # Gamma is the Bias-factor\n",
    "svmclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(svmclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# [0.81343284 0.8358209  0.82462687] # when C=0.75, kernel='rbf', gamma=0.7\n",
    "# [0.8358209  0.83955224 0.84701493] # when C=2.5, kernel='rbf', gamma=0.05 (improved my ranking in Kaggle by 2722 places)\n",
    "\n",
    "# Make Predictions\n",
    "svm_results = svmclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "kernel_name = svmclf.kernel\n",
    "fname = \"data/predictions/svm_with_{0}_kernel_tuned.csv\".format(kernel_name)\n",
    "results2csv(test_x.index, svm_results, fname)\n",
    "\n",
    "confusion_matrix(train_y, svmclf.predict(train_xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= 1.0 # Cost of mis-classification\n",
    "svmclf = svm.SVC(kernel='rbf', gamma=0.1, C=C, random_state=42) # Gamma is the Bias-factor\n",
    "svmclf.fit(train_xx,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(svmclf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# [0.81343284 0.8358209  0.82462687] # when C=0.75, gamma=0.7\n",
    "# [0.8358209  0.83955224 0.84701493] # when C=2.5, gamma=0.05, class_weight={1:1.25} (improved my ranking in Kaggle by 2722 places)\n",
    "# [0.80246914 0.80246914 0.77777778] # when C=100, gamma=0.3 # Scored 0.76076, equivalent to LR in Kaggle. Bad!\n",
    "# [0.82462687 0.8358209  0.85074627] # when C=1.0, gamma=0.1\n",
    "\n",
    "# Make Predictions\n",
    "svm_results = svmclf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "kernel_name = svmclf.kernel\n",
    "fname = \"data/predictions/svm_with_{0}_kernel_tuned2.csv\".format(kernel_name)\n",
    "results2csv(test_x.index, svm_results, fname)\n",
    "\n",
    "confusion_matrix(train_y, svmclf.predict(train_xx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11 : Data Modelling with SVM using RBF Kernel and Grid Search for tuning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_c = range_simple(0.1,2.1,0.1) + [3,5]\n",
    "tpl_gamma = tuple(range_simple(0.01,1.01,0.01,decimal=2))\n",
    "len(tpl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf',svm.SVC(kernel='rbf', random_state=42))\n",
    "])\n",
    "\n",
    "'''\n",
    "params = {\n",
    "    'clf__C':(0.1,0.5,1,2,3,5,10),\n",
    "    'clf__gamma':(0.01,0.1,0.2,0.3,0.5,0.7,0.9,1.0)\n",
    "}\n",
    "'''\n",
    "\n",
    "params = {\n",
    "    'clf__C':tuple(range_simple(0.1,2.1,0.1) + [3,5]),\n",
    "    'clf__gamma':tuple(range_simple(0.00,1.01,0.05,decimal=2))\n",
    "}\n",
    "\n",
    "grid_svm_rbf = GridSearchCV(pipeline,\n",
    "                           params,\n",
    "                           n_jobs=-1, # Use all cores of the machine\n",
    "                           cv=3,\n",
    "                           verbose=1,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "# select_colns = ['Pclass','RoundedFare', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "# select_colns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "select_colns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "train_xx = transform(train_x, select_colns)\n",
    "test_xx = transform(test_x, select_colns)\n",
    "\n",
    "grid_svm_rbf.fit(train_xx, train_y)\n",
    "best_score = grid_svm_rbf.best_score_\n",
    "print('Best Score : ', best_score)\n",
    "\n",
    "best_params = grid_svm_rbf.best_estimator_.get_params()\n",
    "for k in sorted(params.keys()):\n",
    "    print('\\t{0} \\t {1:.2f}'.format(k, best_params[k]))\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(grid_svm_rbf, train_xx, train_y, cv=cv, scoring='accuracy')\n",
    "print('cv_scores :',cv_scores)\n",
    "# cv_scores : [0.80970149, 0.8358209 , 0.85074627] # when all cols selected\n",
    "# cv_scores : [0.80223881 0.83208955 0.84701493] # When cols - RoundedFare, SibSp, Parch are removed\n",
    "# cv_scores : [0.80223881 0.83208955 0.84701493] # when col - RoundedFare is removed\n",
    "# cv_scores : [0.79850746 0.83208955 0.85074627]\n",
    "\n",
    "# Make Predictions\n",
    "svm_results = grid_svm_rbf.predict(test_xx)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/svm_with_svm_rbf_optimized_by_grid_search.csv\"\n",
    "results2csv(test_x.index, svm_results, fname)\n",
    "\n",
    "confusion_matrix(train_y, grid_svm_rbf.predict(train_xx))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12 : Visualizing SVM Classifier with Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "select_colns = ['Pclass', 'Age']\n",
    "# sc.fit(train_x[select_colns])\n",
    "# train_xx = sc.transform(train_x[select_colns])\n",
    "train_xx = train_xx.loc[:,select_colns]\n",
    "\n",
    "C= 1.0 # Cost of mis-classification\n",
    "svmclf = svm.SVC(kernel='rbf', gamma=0.1, C=C, random_state=42) # Gamma is the Bias-factor\n",
    "svmclf.fit(train_xx,train_y)\n",
    "\n",
    "xx, yy = make_meshgrid(train_x.Pclass, train_x.Age) # Because we can only take 2 features to plot it in a 2-d plane\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "# fig, sub = plt.subplots(2, 2)\n",
    "# plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 10)) # ax = subplot\n",
    "plot_contours(ax, svmclf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "ax.scatter(train_x.Pclass, train_x.Age, c=train_y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "# ax.set_xlim(xx.min(), xx.max())\n",
    "# ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xlabel('P-Class')\n",
    "ax.set_ylabel('Age')\n",
    "# ax.set_xticks(())\n",
    "# ax.set_yticks(())\n",
    "ax.set_title(\"Grid Search with SVM RBF Classifier\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13 : Data Modelling with Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtclf = tree.DecisionTreeClassifier(random_state=42)\n",
    "dtclf.fit(train_x,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(dtclf, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# [0.76492537 0.7761194  0.81716418] # With default params\n",
    "\n",
    "# Make Predictions\n",
    "dt_results = dtclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/decision_tree.csv\"\n",
    "results2csv(test_x.index, dt_results, fname)\n",
    "\n",
    "confusion_matrix(train_y, dtclf.predict(train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Decision Tree graph\n",
    "# import sys\n",
    "# sys.path.append('C:\\graphviz-2.38\\bin')\n",
    "# !conda install graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(dtclf,\n",
    "                                out_file='images/tree.dot',\n",
    "                                feature_names=train_x.columns.tolist(),\n",
    "                                class_names=['Dead','Survived'],\n",
    "                                rounded=True,\n",
    "                                filled=True)\n",
    "\n",
    "# Uncomment line below to write the output to the file in the disk\n",
    "# ! dot -Tpng images/tree.dot -o images/tree.png\n",
    "\n",
    "'''\n",
    "# The method spits out a \".dot\" extension file for the graph. \n",
    "# Essentialy this format data is referenced in dot_data variable and later used to show the tree graph\n",
    "dot_data = tree.export_graphviz(dtclf,\n",
    "                                out_file=None,\n",
    "                                feature_names=train_x.columns.tolist(),\n",
    "                                class_names=['Dead','Survived'],\n",
    "#                                 proportion=True,\n",
    "                                rounded=True,\n",
    "                                filled=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.format = 'png'\n",
    "graph\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14 : Data Modelling with Optimized Decision Tree Classifier tuned using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odtclf = tree.DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "params = {\n",
    "    'max_leaf_nodes' : list(range(2,50)),\n",
    "    'min_samples_split' : [2,3,4],\n",
    "    'min_samples_leaf' : list(range(1,22,2)),\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "gsclf = GridSearchCV(odtclf, params, n_jobs=-1,cv=cv,verbose=1,scoring='accuracy')\n",
    "gsclf.fit(train_x,train_y)\n",
    "cv_scores = cross_val_score(gsclf, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# CV Scores : [0.79850746 0.80970149 0.84701493]\n",
    "\n",
    "print('Best Score : ', gsclf.best_score_)\n",
    "print(gsclf.best_estimator_)\n",
    "\n",
    "# Print the key/value pairs of best params\n",
    "best_params = gsclf.best_estimator_.get_params()\n",
    "for k in sorted(params.keys()):\n",
    "    print('\\t{0} \\t {1}'.format(k, best_params[k]))\n",
    "\n",
    "# Make Predictions    \n",
    "test_y_pred = gsclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/decision_tree_optimized_by_grid_search.csv\"\n",
    "results2csv(test_x.index, test_y_pred, fname)\n",
    "\n",
    "confusion_matrix(train_y, gsclf.predict(train_x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist Python Object : The  Optimized Decision Tree Classifier, for use in advancecd modelling\n",
    "pickle_in(odtclf, 'pickle/optimized_dtree_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
