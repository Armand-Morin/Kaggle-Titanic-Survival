{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting Prediction with Random Forests, Extra-Trees, VotingClassifier Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "# Metrics and Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Custom Functions\n",
    "from util.author import results2csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Precessed Data\n",
    "train = pd.read_csv('data/train_processed_1.csv', index_col='PassengerId')\n",
    "test = pd.read_csv('data/test_processed_1.csv', index_col='PassengerId')\n",
    "\n",
    "# Feature Engineering\n",
    "train = pd.get_dummies(data=train, drop_first=True)\n",
    "test = pd.get_dummies(data=test, drop_first=True)\n",
    "\n",
    "# Split Datasets\n",
    "train_y = train.pop('Survived')\n",
    "train_x = train\n",
    "test_x = test # Nothing to split! Test-set has no target columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForests Ensemble\n",
    "* Built on top of Decision Trees.\n",
    "* In general, the more trees in the forest, the more stable the prediction and thus high accuracy.\n",
    "* How it works?\n",
    "  - Grow more trees\n",
    "  - A Tree of maximal depth is grown on a bootstrap sample of size m of the training set. There is no pruning.\n",
    "  - A number m << p is specified such that at each node,\n",
    "      m variables are sampled at random, out of p.\n",
    "      The best-split of these variables, is used to split the node into 2 sub-nodes.\n",
    "  - Final classification is given by majority voting of the ensemble of trees in the forest.\n",
    "  - There are only 2 \"free parameters\":\n",
    "      1. number of trees, and\n",
    "      2. number of variables in random subset at each node.\n",
    "  - For given input if it haas to be classified, then the vote from each tree is collected and the class with majority votes is chosen by the RandomForest algorithm.\n",
    "* Advantages\n",
    "  - RandomForest can handle missing values and maintain accuracy for missing data.\n",
    "  - Won't overfit the model\n",
    "  - It can handle large dataset with higher dimensionality\n",
    "* Disadvantages\n",
    "  - Not as good for Regression as it is for Classification\n",
    "  - You have very little control of what the model does. It's a black-box for statistical modedlling.\n",
    "* Known application/use-cases for RF\n",
    "  - Classifying medicines\n",
    "  - Identify diseases\n",
    "  - Anticipate stock-behaviour in stock market.\n",
    "  - In ecommerce, used for product recommendations.\n",
    "  - Image classification.\n",
    "  - MS has used this for body-parts idedntification in X-Box connect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfclf = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=500)\n",
    "rfclf.fit(train_x,train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(rfclf, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# CV Scores : [0.78358209 0.79104478 0.80970149] # Default params\n",
    "# CV Scores : [0.79477612 0.79850746 0.79850746] # n_estimators=50\n",
    "# CV Scores : [0.78358209 0.79477612 0.79850746] # n_estimators=100\n",
    "# CV Scores : [0.78731343 0.79850746 0.80597015] # n_estimators=500\n",
    "\n",
    "# Make Predictions    \n",
    "test_y_pred = rfclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/random_forest.csv\"\n",
    "results2csv(test_x.index, test_y_pred, fname)\n",
    "\n",
    "confusion_matrix(train_y, rfclf.predict(train_x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfclf = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "params = {\n",
    "    'n_estimators':[10,20,50,100],\n",
    "    'min_samples_leaf':[1,2,3,4,5],\n",
    "    'min_samples_split': [.2,.25, .3,.4,.5], # [2,3,5,7,11,13],\n",
    "    'max_features':[2,3,4,5,6],\n",
    "    'max_depth':[None,7,6,5]\n",
    "}\n",
    "\n",
    "gsclf = GridSearchCV(rfclf, \n",
    "                     n_jobs=-1, # Use all cores of the machine\n",
    "                     param_grid=params,\n",
    "                     cv=3,\n",
    "                     verbose=1, \n",
    "                     scoring='accuracy')\n",
    "gsclf.fit(train_x, train_y)\n",
    "\n",
    "best_score = gsclf.best_score_\n",
    "print('Best Score : ', best_score)\n",
    "# print('OOB Score : ', gsclf.oob_score_)\n",
    "'''\n",
    "Best Score :  0.8282828282828283\n",
    "\tmax_depth \t 6\n",
    "\tmin_samples_leaf \t 3\n",
    "\tmin_samples_split \t 2\n",
    "\tn_estimators \t 20\n",
    "\n",
    "Best Score :  0.8092031425364759\n",
    "\tmax_depth \t 5\n",
    "\tmax_features \t 2\n",
    "\tmin_samples_leaf \t 3\n",
    "\tmin_samples_split \t 0.2\n",
    "\tn_estimators \t 20\n",
    "'''\n",
    "\n",
    "# gsclf.get_params()\n",
    "best_params = gsclf.best_estimator_.get_params()\n",
    "for k in sorted(params.keys()):\n",
    "    print('\\t{0} \\t {1}'.format(k, best_params[k]))\n",
    "\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(gsclf, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# CV Scores : [0.78358209 0.79104478 0.80970149] # Default params\n",
    "# CV Scores : [0.82089552 0.80970149 0.82835821] # With best_params\n",
    "# CV Scores : [0.80970149 0.81343284 0.81716418] # With best_params\n",
    "\n",
    "# Make Predictions    \n",
    "test_y_pred = gsclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/random_forest_tuned.csv\"\n",
    "results2csv(test_x.index, test_y_pred, fname)\n",
    "\n",
    "confusion_matrix(train_y, gsclf.predict(train_x))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra-Trees (Extremely Randomized Trees) Ensemble\n",
    "* ExtraTrees make use of random thresholds for each feature. This is how it differs from Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtclf = ExtraTreesClassifier(random_state=42, n_jobs=-1, bootstrap=True)\n",
    "xtclf.fit(train_x, train_y)\n",
    "\n",
    "# Measuring Accuracy with K-fold Cross-Validation\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n",
    "cv_scores = cross_val_score(xtclf, train_x, train_y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores :', cv_scores)\n",
    "# CV Scores : [0.75       0.78358209 0.79104478] # Default params\n",
    "# CV Scores : [0.7761194  0.79104478 0.77985075] # When bootstrap=True\n",
    "\n",
    "\n",
    "# Make Predictions    \n",
    "test_y_pred = xtclf.predict(test_x)\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "fname = \"data/predictions/xtra_trees.csv\"\n",
    "results2csv(test_x.index, test_y_pred, fname)\n",
    "\n",
    "confusion_matrix(train_y, xtclf.predict(train_x))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifier Ensemble\n",
    "Ref.: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "Voting Types:\n",
    "* Hard (default) : If ‘hard’, uses predicted class labels for majority rule voting.\n",
    "* Soft : If ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm, tree\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "lrclf = LogisticRegression(random_state=42, max_iter=300, C=0.3, solver='sag',n_jobs=3)\n",
    "sgdclf = SGDClassifier(random_state=42, max_iter=1000, alpha=0.7)\n",
    "svmclf = svm.SVC(kernel='rbf', gamma=0.7, C=0.75, random_state=42, probability=True)\n",
    "dtclf = tree.DecisionTreeClassifier(max_leaf_nodes=47, min_samples_split=4, random_state=42)\n",
    "rfclf = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True, max_depth=6, min_samples_leaf=3, min_samples_split=2, n_estimators=20)\n",
    "\n",
    "vc1 = VotingClassifier(estimators=[('lrclf',lrclf), ('sgdclf',sgdclf), ('svmclf',svmclf), ('dtclf',dtclf), ('rfclf',rfclf)],\n",
    "                      voting='hard')\n",
    "vc1.fit(train_x, train_y)\n",
    "vc2 = VotingClassifier(estimators=[('lrclf',lrclf), \n",
    "#                                    ('sgdclf',sgdclf), \n",
    "                                   ('svmclf',svmclf), \n",
    "                                   ('dtclf',dtclf), \n",
    "                                   ('rfclf',rfclf)],\n",
    "                      voting='soft')\n",
    "vc2.fit(train_x, train_y)\n",
    "\n",
    "test_y_pred_1 = vc1.predict(test_x)\n",
    "test_y_pred_2 = vc2.predict(test_x)\n",
    "\n",
    "print(confusion_matrix(train_y, vc1.predict(train_x)))\n",
    "'''\n",
    "[[539  10]\n",
    " [110 232]]\n",
    "'''\n",
    "print(confusion_matrix(train_y, vc2.predict(train_x)))\n",
    "'''\n",
    "[[529  20]\n",
    " [ 67 275]]\n",
    "'''\n",
    "\n",
    "# Persist Data to CSV file for submission\n",
    "results2csv(test_x.index, test_y_pred_1, \"data/predictions/voting_classifier_hard.csv\")\n",
    "results2csv(test_x.index, test_y_pred_2, \"data/predictions/voting_classifier_soft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
